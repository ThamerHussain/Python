{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_01_Tokenizer - Thamer Hussain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Text File \n",
        "\n"
      ],
      "metadata": {
        "id": "XAHqJPlbgm-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/TheAIDojo/Machine_Learning_Bootcamp/main/Week%2001-%20%20Introduction%20to%20Python/text_data.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly8yQ4rwgzLp",
        "outputId": "c08a5aef-3fc8-49b5-ff59-3056995730b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-18 18:29:53--  https://raw.githubusercontent.com/TheAIDojo/Machine_Learning_Bootcamp/main/Week%2001-%20%20Introduction%20to%20Python/text_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11916 (12K) [text/plain]\n",
            "Saving to: ‘text_data.txt’\n",
            "\n",
            "\rtext_data.txt         0%[                    ]       0  --.-KB/s               \rtext_data.txt       100%[===================>]  11.64K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-18 18:29:53 (44.6 MB/s) - ‘text_data.txt’ saved [11916/11916]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the Text File \n"
      ],
      "metadata": {
        "id": "P8i7ekH3O5_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data from the text file \n",
        "with open('/content/text_data.txt','r') as f:\n",
        "    row_text=f.read()\n"
      ],
      "metadata": {
        "id": "6SPs5CAMPKJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Tokenizer Class "
      ],
      "metadata": {
        "id": "LshIeWf1OpXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        row_text,\n",
        "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "        \"\"\"\n",
        "        The __init__ method  of the class Tokenizer will get the following parameters\n",
        "        row_text : the input text that we wnat to create the tokenizer from it.\n",
        "        filters : a string where each element is a character that will be filtered from the texts. \n",
        "        The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
        "         \"\"\"\n",
        "\n",
        "        self.row_text = row_text\n",
        "        self.filters = filters\n",
        "\n",
        "        # create the word to index and index to word dictionaries using the tokenizer method\n",
        "\n",
        "        # start of the code\n",
        "        dictions = self.tokenizer(self.row_text)\n",
        "        self.word_index = dictions[0]\n",
        "        self.index_word = dictions[1]\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        The clean_text method will take one parameter text \n",
        "        then the method will clean the text data \n",
        "        and return clean text\n",
        "        \"\"\"\n",
        "\n",
        "        text = text.lower()  # convert the texts to lowercase.\n",
        "\n",
        "        # clean the text\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for x in self.filters :\n",
        "           text = text.replace(x, '')\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "        return text  # return the clean text\n",
        "\n",
        "    def tokenizer(self, text):\n",
        "        \"\"\"\n",
        "        The tokenizer method will take one parameter text \n",
        "        then the method will clean the text data. \n",
        "        extricate the unique words the method will return two dictionaries contains the unique word to index and the reverse of it.\n",
        "        \"\"\"\n",
        "\n",
        "        # create the word to index dictionary\n",
        "\n",
        "        word_index = {}\n",
        "\n",
        "        # create the index to word dictionary\n",
        "\n",
        "        index_word = {}\n",
        "\n",
        "       \n",
        "\n",
        "        # clean the text\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "\n",
        "        self.clean_text(text)\n",
        "\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # extract the unique values\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        unique = list(set(text.split(\" \")))\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # convert the word to index and the reverse of it\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for x in range(len(unique)):\n",
        "          word_index[unique[x]] = x\n",
        "          index_word[x] = unique[x]\n",
        "        \n",
        "        # end of the code\n",
        "\n",
        "\n",
        "        # return word to index and the indeex to word dictionaries\n",
        "\n",
        "        return (word_index, index_word)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"\n",
        "        the method tokenize will convert text to vector of integers  \n",
        "        text: the text paramater it's for the text that you want to convert to list of numbers\n",
        "        \"\"\"\n",
        "\n",
        "        # create empty  list\n",
        "\n",
        "        vector = []\n",
        "\n",
        "\n",
        "\n",
        "        # clean the text\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        text = self.clean_text(text)\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "        # split the text to words\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        unique = list(set(text.split(\" \")))\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "\n",
        "\n",
        "        # convert the word list to\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for x in unique:\n",
        "          if x in self.word_index:\n",
        "            vector.append(self.word_index[x])\n",
        "\n",
        "        # end of the code\n",
        "            \n",
        "\n",
        "        # return the  vector\n",
        "        #return vector\n",
        "        print(vector)\n",
        "        print(text)\n",
        "\n",
        "    def reverse_tokenize(self, vector):\n",
        "        \"\"\"\n",
        "        the method reverse_tokenize will convert vector of integers to text\n",
        "        vector: the vector paramater it's for the list of number that you want to convert to string\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        word_list = []\n",
        "\n",
        "        # convert from tokenize to string\n",
        "\n",
        "        # start of the code\n",
        "\n",
        "        for x in vector:\n",
        "          word_list.append(self.index_word[x])\n",
        "\n",
        "        # end of the code\n",
        "\n",
        "        # return the string\n",
        "        #return word_list\n",
        "        print(vector)\n",
        "        print(word_list)"
      ],
      "metadata": {
        "id": "kPbBX2ZTOiHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tokenizer object \n",
        "\n",
        "X = tokenizer = Tokenizer(row_text)\n",
        "X.tokenize(test_1)"
      ],
      "metadata": {
        "id": "wjKqhKD4WQad",
        "outputId": "57ad2137-771f-43a3-c3c9-bbed2fc4ff5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[383, 61, 844, 788, 269, 727, 144, 147, 349, 409, 466, 517, 741, 571, 576, 419, 359, 938, 39, 582, 753, 521, 524, 306, 49, 528, 589, 704, 648, 832, 249, 770, 434, 254, 838]\n",
            "care for us true indeed they ne'er cared for usyet suffer us to famish and their store  houses crammed with grain make edicts for usury to support usurers repeal daily any wholesome act established against the rich and provide more piercing statutes daily to chain up and restrain the poor if the wars eat us not up they will and there's all the love they bear us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Tokenizer"
      ],
      "metadata": {
        "id": "4Qj-QcZpCWmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_1 = \"\"\"\n",
        "Care for us! True, indeed! They ne'er cared for us\n",
        "yet: suffer us to famish, and their store - houses \n",
        "crammed with grain; make edicts for usury, to \n",
        "support usurers; repeal daily any wholesome act \n",
        "established against the rich, and provide more \n",
        "piercing statutes daily, to chain up and restrain \n",
        "the poor. If the wars eat us not up, they will; and \n",
        "there's all the love they bear us.\"\"\"\n",
        "\n",
        "test_2 = \"\"\"Well, I'll hear it, sir: yet you must not think to\n",
        "fob off our disgrace with a tale: but, an 't please\n",
        "you, deliver.\"\"\""
      ],
      "metadata": {
        "id": "pyG0ccsXWX2M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}